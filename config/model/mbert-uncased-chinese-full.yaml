name: bert-base-multilingual-uncased
class_name: AutoModelForSequenceClassification
tokenizer_class: AutoTokenizer
tokenizer_name: bert-base-multilingual-uncased
inner_params:
- bert.encoder.layer.0.attention.self.query.weight
- bert.encoder.layer.0.attention.self.key.weight
- bert.encoder.layer.0.attention.self.value.weight
- bert.encoder.layer.0.attention.output.dense.weight
- bert.encoder.layer.0.intermediate.dense.weight
- bert.encoder.layer.0.output.dense.weight
- bert.encoder.layer.1.attention.self.query.weight
- bert.encoder.layer.1.attention.self.key.weight
- bert.encoder.layer.1.attention.self.value.weight
- bert.encoder.layer.1.attention.output.dense.weight
- bert.encoder.layer.1.intermediate.dense.weight
- bert.encoder.layer.1.output.dense.weight
- bert.encoder.layer.2.attention.self.query.weight
- bert.encoder.layer.2.attention.self.key.weight
- bert.encoder.layer.2.attention.self.value.weight
- bert.encoder.layer.2.attention.output.dense.weight
- bert.encoder.layer.2.intermediate.dense.weight
- bert.encoder.layer.2.output.dense.weight
- bert.encoder.layer.3.attention.self.query.weight
- bert.encoder.layer.3.attention.self.key.weight
- bert.encoder.layer.3.attention.self.value.weight
- bert.encoder.layer.3.attention.output.dense.weight
- bert.encoder.layer.3.intermediate.dense.weight
- bert.encoder.layer.3.output.dense.weight
- bert.encoder.layer.4.attention.self.query.weight
- bert.encoder.layer.4.attention.self.key.weight
- bert.encoder.layer.4.attention.self.value.weight
- bert.encoder.layer.4.attention.output.dense.weight
- bert.encoder.layer.4.intermediate.dense.weight
- bert.encoder.layer.4.output.dense.weight
- bert.encoder.layer.5.attention.self.query.weight
- bert.encoder.layer.5.attention.self.key.weight
- bert.encoder.layer.5.attention.self.value.weight
- bert.encoder.layer.5.attention.output.dense.weight
- bert.encoder.layer.5.intermediate.dense.weight
- bert.encoder.layer.5.output.dense.weight
- bert.encoder.layer.6.attention.self.query.weight
- bert.encoder.layer.6.attention.self.key.weight
- bert.encoder.layer.6.attention.self.value.weight
- bert.encoder.layer.6.attention.output.dense.weight
- bert.encoder.layer.6.intermediate.dense.weight
- bert.encoder.layer.6.output.dense.weight
- bert.encoder.layer.7.attention.self.query.weight
- bert.encoder.layer.7.attention.self.key.weight
- bert.encoder.layer.7.attention.self.value.weight
- bert.encoder.layer.7.attention.output.dense.weight
- bert.encoder.layer.7.intermediate.dense.weight
- bert.encoder.layer.7.output.dense.weight
- bert.encoder.layer.8.attention.self.query.weight
- bert.encoder.layer.8.attention.self.key.weight
- bert.encoder.layer.8.attention.self.value.weight
- bert.encoder.layer.8.attention.output.dense.weight
- bert.encoder.layer.8.intermediate.dense.weight
- bert.encoder.layer.8.output.dense.weight
- bert.encoder.layer.9.attention.self.query.weight
- bert.encoder.layer.9.attention.self.key.weight
- bert.encoder.layer.9.attention.self.value.weight
- bert.encoder.layer.9.attention.output.dense.weight
- bert.encoder.layer.9.intermediate.dense.weight
- bert.encoder.layer.9.output.dense.weight
- bert.encoder.layer.10.attention.self.query.weight
- bert.encoder.layer.10.attention.self.key.weight
- bert.encoder.layer.10.attention.self.value.weight
- bert.encoder.layer.10.attention.output.dense.weight
- bert.encoder.layer.10.intermediate.dense.weight
- bert.encoder.layer.10.output.dense.weight
- bert.encoder.layer.11.attention.self.query.weight
- bert.encoder.layer.11.attention.self.key.weight
- bert.encoder.layer.11.attention.self.value.weight
- bert.encoder.layer.11.attention.output.dense.weight
- bert.encoder.layer.11.intermediate.dense.weight
- bert.encoder.layer.11.output.dense.weight


pt:  /home/anonymous-xme/bloom-finetune/results/mbert-uncased/finetuning-fever-1L-chinese-lr15-wt13-pred/checkpoint-3281/pytorch_model.bin
